{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81fcb893",
   "metadata": {},
   "source": [
    "# HMM Experiment example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cc365a",
   "metadata": {},
   "source": [
    "In this example, we will use the HMM dataset provided by this package, get some attributions,\n",
    "and evaluate them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8131df67",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf20266a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "\n",
    "from tint.attr import TemporalAugmentedOcclusion, TimeForwardTunnel\n",
    "from tint.datasets import HMM\n",
    "from tint.metrics.white_box import aur\n",
    "\n",
    "from main import main\n",
    "from classifier import StateClassifierNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00d49de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92ea4b975a71490d81a3ad07437a8d29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lime attribution:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\20191937\\Documents\\GitHub\\factai\\experiments\\hmm\\main.py:253\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(explainers, device, fold, seed, deterministic, lambda_1, lambda_2, output_file, rerun_all)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlime\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m explainers:\n\u001b[0;32m    252\u001b[0m     explainer \u001b[38;5;241m=\u001b[39m TimeForwardTunnel(Lime(classifier))\n\u001b[1;32m--> 253\u001b[0m     attr[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlime\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattribute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbinary\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mabs()\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maugmented_occlusion\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m explainers:\n\u001b[0;32m    260\u001b[0m     explainer \u001b[38;5;241m=\u001b[39m TimeForwardTunnel(\n\u001b[0;32m    261\u001b[0m         TemporalAugmentedOcclusion(\n\u001b[0;32m    262\u001b[0m             classifier, data\u001b[38;5;241m=\u001b[39mx_train, n_sampling\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, is_temporal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    263\u001b[0m         )\n\u001b[0;32m    264\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\20191937\\.conda\\envs\\tint\\lib\\site-packages\\captum\\log\\__init__.py:42\u001b[0m, in \u001b[0;36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\20191937\\Documents\\GitHub\\factai\\experiments\\hmm\\tint\\attr\\time_forward_tunnel.py:203\u001b[0m, in \u001b[0;36mTimeForwardTunnel.attribute\u001b[1;34m(self, inputs, task, threshold, temporal_target, temporal_additional_forward_args, return_temporal_attributions, show_progress, **kwargs)\u001b[0m\n\u001b[0;32m    197\u001b[0m delta_partial_list_sublist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m partial_target \u001b[38;5;129;01min\u001b[39;00m partial_targets:\n\u001b[0;32m    199\u001b[0m     (\n\u001b[0;32m    200\u001b[0m         attributions_partial,\n\u001b[0;32m    201\u001b[0m         is_attrib_tuple,\n\u001b[0;32m    202\u001b[0m         delta_partial,\n\u001b[1;32m--> 203\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_partial_attribution\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartial_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartial_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartial_target\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartial_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_inputs_tuple\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_inputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_convergence_delta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_convergence_delta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs_partition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs_copy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    210\u001b[0m     attributions_partial_sublist\u001b[38;5;241m.\u001b[39mappend(attributions_partial)\n\u001b[0;32m    211\u001b[0m     delta_partial_list_sublist\u001b[38;5;241m.\u001b[39mappend(delta_partial)\n",
      "File \u001b[1;32mc:\\Users\\20191937\\Documents\\GitHub\\factai\\experiments\\hmm\\tint\\attr\\time_forward_tunnel.py:285\u001b[0m, in \u001b[0;36mTimeForwardTunnel.compute_partial_attribution\u001b[1;34m(self, partial_inputs, partial_target, is_inputs_tuple, return_convergence_delta, kwargs_partition)\u001b[0m\n\u001b[0;32m    279\u001b[0m     attributions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattribution_method\u001b[38;5;241m.\u001b[39mattribute\u001b[38;5;241m.\u001b[39m__wrapped__(\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattribution_method,  \u001b[38;5;66;03m# self\u001b[39;00m\n\u001b[0;32m    281\u001b[0m         partial_inputs \u001b[38;5;28;01mif\u001b[39;00m is_inputs_tuple \u001b[38;5;28;01melse\u001b[39;00m partial_inputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_partition,\n\u001b[0;32m    283\u001b[0m     )\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 285\u001b[0m     attributions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattribution_method\u001b[38;5;241m.\u001b[39mattribute\u001b[38;5;241m.\u001b[39m__wrapped__(\n\u001b[0;32m    286\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattribution_method,  \u001b[38;5;66;03m# self\u001b[39;00m\n\u001b[0;32m    287\u001b[0m         partial_inputs \u001b[38;5;28;01mif\u001b[39;00m is_inputs_tuple \u001b[38;5;28;01melse\u001b[39;00m partial_inputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    288\u001b[0m         target\u001b[38;5;241m=\u001b[39mpartial_target,\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_partition,\n\u001b[0;32m    290\u001b[0m     )\n\u001b[0;32m    291\u001b[0m delta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_delta_supported \u001b[38;5;129;01mand\u001b[39;00m return_convergence_delta:\n",
      "File \u001b[1;32mc:\\Users\\20191937\\.conda\\envs\\tint\\lib\\site-packages\\captum\\attr\\_core\\lime.py:1061\u001b[0m, in \u001b[0;36mLime.attribute\u001b[1;34m(self, inputs, baselines, target, additional_forward_args, feature_mask, n_samples, perturbations_per_eval, return_input_shape, show_progress)\u001b[0m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;129m@log_usage\u001b[39m()\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mattribute\u001b[39m(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    833\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    842\u001b[0m     show_progress: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    843\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TensorOrTupleOfTensorsGeneric:\n\u001b[0;32m    844\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    845\u001b[0m \u001b[38;5;124;03m    This method attributes the output of the model with given target index\u001b[39;00m\n\u001b[0;32m    846\u001b[0m \u001b[38;5;124;03m    (in case it is provided, otherwise it assumes that output is a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1059\u001b[0m \u001b[38;5;124;03m        >>> attr = lime.attribute(input, target=1, feature_mask=feature_mask)\u001b[39;00m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1061\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attribute_kwargs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1062\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1063\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbaselines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaselines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1064\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mperturbations_per_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mperturbations_per_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_input_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_input_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1070\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\20191937\\.conda\\envs\\tint\\lib\\site-packages\\captum\\attr\\_core\\lime.py:1130\u001b[0m, in \u001b[0;36mLime._attribute_kwargs\u001b[1;34m(self, inputs, baselines, target, additional_forward_args, feature_mask, n_samples, perturbations_per_eval, return_input_shape, show_progress, **kwargs)\u001b[0m\n\u001b[0;32m   1115\u001b[0m output_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1116\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (\n\u001b[0;32m   1117\u001b[0m     curr_inps,\n\u001b[0;32m   1118\u001b[0m     curr_target,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1128\u001b[0m     feature_mask,\n\u001b[0;32m   1129\u001b[0m ):\n\u001b[1;32m-> 1130\u001b[0m     coefs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mattribute\u001b[38;5;241m.\u001b[39m__wrapped__(\n\u001b[0;32m   1131\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1132\u001b[0m         inputs\u001b[38;5;241m=\u001b[39mcurr_inps \u001b[38;5;28;01mif\u001b[39;00m is_inputs_tuple \u001b[38;5;28;01melse\u001b[39;00m curr_inps[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   1133\u001b[0m         target\u001b[38;5;241m=\u001b[39mcurr_target,\n\u001b[0;32m   1134\u001b[0m         additional_forward_args\u001b[38;5;241m=\u001b[39mcurr_additional_args,\n\u001b[0;32m   1135\u001b[0m         n_samples\u001b[38;5;241m=\u001b[39mn_samples,\n\u001b[0;32m   1136\u001b[0m         perturbations_per_eval\u001b[38;5;241m=\u001b[39mperturbations_per_eval,\n\u001b[0;32m   1137\u001b[0m         baselines\u001b[38;5;241m=\u001b[39mcurr_baselines\n\u001b[0;32m   1138\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m is_inputs_tuple\n\u001b[0;32m   1139\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m curr_baselines[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   1140\u001b[0m         feature_mask\u001b[38;5;241m=\u001b[39mcurr_feature_mask\n\u001b[0;32m   1141\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m is_inputs_tuple\n\u001b[0;32m   1142\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m curr_feature_mask[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   1143\u001b[0m         num_interp_features\u001b[38;5;241m=\u001b[39mnum_interp_features,\n\u001b[0;32m   1144\u001b[0m         show_progress\u001b[38;5;241m=\u001b[39mshow_progress,\n\u001b[0;32m   1145\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1146\u001b[0m     )\n\u001b[0;32m   1147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_input_shape:\n\u001b[0;32m   1148\u001b[0m         output_list\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m   1149\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_output_shape(\n\u001b[0;32m   1150\u001b[0m                 curr_inps,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1155\u001b[0m             )\n\u001b[0;32m   1156\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\20191937\\.conda\\envs\\tint\\lib\\site-packages\\captum\\attr\\_core\\lime.py:484\u001b[0m, in \u001b[0;36mLimeBase.attribute\u001b[1;34m(self, inputs, target, additional_forward_args, n_samples, perturbations_per_eval, show_progress, **kwargs)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expanded_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    482\u001b[0m     expanded_target \u001b[38;5;241m=\u001b[39m _expand_target(target, \u001b[38;5;28mlen\u001b[39m(curr_model_inputs))\n\u001b[1;32m--> 484\u001b[0m model_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcurr_model_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpanded_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpanded_additional_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m show_progress:\n\u001b[0;32m    492\u001b[0m     attr_progress\u001b[38;5;241m.\u001b[39mupdate()\n",
      "File \u001b[1;32mc:\\Users\\20191937\\.conda\\envs\\tint\\lib\\site-packages\\captum\\attr\\_core\\lime.py:540\u001b[0m, in \u001b[0;36mLimeBase._evaluate_batch\u001b[1;34m(self, curr_model_inputs, expanded_target, expanded_additional_args, device)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_evaluate_batch\u001b[39m(\n\u001b[0;32m    534\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    535\u001b[0m     curr_model_inputs: List[TensorOrTupleOfTensorsGeneric],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    538\u001b[0m     device: torch\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[0;32m    539\u001b[0m ):\n\u001b[1;32m--> 540\u001b[0m     model_out \u001b[38;5;241m=\u001b[39m \u001b[43m_run_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_reduce_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurr_model_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpanded_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpanded_additional_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    546\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model_out, Tensor):\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m model_out\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(curr_model_inputs), (\n\u001b[0;32m    548\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of outputs is not appropriate, must return \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    549\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mone output per perturbed input\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    550\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\20191937\\.conda\\envs\\tint\\lib\\site-packages\\captum\\_utils\\common.py:531\u001b[0m, in \u001b[0;36m_run_forward\u001b[1;34m(forward_func, inputs, target, additional_forward_args)\u001b[0m\n\u001b[0;32m    528\u001b[0m inputs \u001b[38;5;241m=\u001b[39m _format_inputs(inputs)\n\u001b[0;32m    529\u001b[0m additional_forward_args \u001b[38;5;241m=\u001b[39m _format_additional_forward_args(additional_forward_args)\n\u001b[1;32m--> 531\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mforward_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _select_targets(output, target)\n",
      "File \u001b[1;32mc:\\Users\\20191937\\.conda\\envs\\tint\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\20191937\\.conda\\envs\\tint\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\20191937\\Documents\\GitHub\\factai\\experiments\\hmm\\classifier.py:112\u001b[0m, in \u001b[0;36mStateClassifierNet.forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\20191937\\.conda\\envs\\tint\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\20191937\\.conda\\envs\\tint\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\20191937\\Documents\\GitHub\\factai\\experiments\\hmm\\classifier.py:51\u001b[0m, in \u001b[0;36mStateClassifier.forward\u001b[1;34m(self, x, return_all)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, return_all: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGRU\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 51\u001b[0m         all_encodings, encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m         all_encodings, (encoding, state) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(x)\n",
      "File \u001b[1;32mc:\\Users\\20191937\\.conda\\envs\\tint\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\20191937\\.conda\\envs\\tint\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\20191937\\.conda\\envs\\tint\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:1102\u001b[0m, in \u001b[0;36mGRU.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1102\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1105\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mgru(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m   1106\u001b[0m                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "m = main(['lime'], device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616585d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e13b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.detach().cpu().numpy().dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0347695c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm = HMM(n_folds=5, fold=0, seed=42)\n",
    "true_saliency = hmm.true_saliency(split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5749f275",
   "metadata": {},
   "outputs": [],
   "source": [
    "information(m, true_saliency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6a1117",
   "metadata": {},
   "source": [
    "### Make reproducible experiment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f06292c",
   "metadata": {},
   "source": [
    "For this example, we will make everything reproducible. With this aim, we use the \n",
    "tool from Pytorch-Lightning: seed_everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455bd164",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "seed_everything(seed=seed, workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca50bb6c",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c30d964",
   "metadata": {},
   "source": [
    "We load the HMM (Hidden Markov Model) dataset, and eventually download it (since arma is a synthetic dataset, \n",
    "download actually generates the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb4db39",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm = HMM(seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2918d99e",
   "metadata": {},
   "source": [
    "### Create and train a simple classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b4067b",
   "metadata": {},
   "source": [
    "We will now train a simple classifier (a GRU followed by a MLP) over the HMM dataset.\n",
    "This dataset provides indeed labels, generated given the hidden states of the HMM.\n",
    "\n",
    "We use the Pytorch-Lightning framework to efficiently train this model. Here, the \n",
    "accelerator is set to ``cpu``, but feel free to change this to ``gpu``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50076fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = \"gpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0beb273",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = StateClassifierNet(\n",
    "    feature_size=3,\n",
    "    n_state=2,\n",
    "    hidden_size=200,\n",
    "    regres=True,\n",
    "    loss=\"cross_entropy\",\n",
    "    lr=0.0001,\n",
    "    l2=1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f058fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "th.use_deterministic_algorithms(True)\n",
    "trainer = Trainer(\n",
    "    max_epochs=50, accelerator=accelerator, deterministic=True\n",
    ")\n",
    "trainer.fit(classifier, datamodule=hmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf7771d",
   "metadata": {},
   "source": [
    "### Get train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d77876",
   "metadata": {},
   "source": [
    "We only compute the attributions over the test set. However, we will also need the train data here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db47494",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = hmm.preprocess(split=\"train\")[\"x\"].to(accelerator)\n",
    "x_test = hmm.preprocess(split=\"test\")[\"x\"].to(accelerator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a052b62",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde710ae",
   "metadata": {},
   "source": [
    "We set the classifer to the ``evaluation`` mode, and we push it to the current accelerator.\n",
    "\n",
    "We also disable ``cudnn`` when using ``cuda``, as it cannot backpropagate when set on evaluation.\n",
    "Please refer to https://captum.ai/docs/faq#how-can-i-resolve-cudnn-rnn-backward-error-for-rnn-or-lstm-network \n",
    "for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621ed3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to eval\n",
    "classifier.eval()\n",
    "\n",
    "# Set model to accelerator\n",
    "classifier.to(accelerator)\n",
    "\n",
    "if accelerator == \"cuda\":\n",
    "    th.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b058fb4",
   "metadata": {},
   "source": [
    "### Create attributions using temporal augmented occlusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f283cf",
   "metadata": {},
   "source": [
    "In this example, we will use ``temporal_augmented_occlusion`` as an attribution method, first presented \n",
    "in this paper: https://arxiv.org/abs/2003.02821. This method hides some data like the ``Occlusion`` method,\n",
    "however, instead of replacing the hidden data with a baseline, it samples this baseline\n",
    "from a bootstrapped distribution. Moreover, unlike the regular ``augmented_occlusion``, this method only \n",
    "hides data from the last time, leaving the past data unchanged.\n",
    "\n",
    "We also use a special tool: ``TimeForwardTunnel``. This method allows us to compute attributions\n",
    "at each different time using only the past as information. The ``TimeForwardTunnel`` then loops over \n",
    "every time to compute every attributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235e53be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "explainer = TimeForwardTunnel(\n",
    "    TemporalAugmentedOcclusion(\n",
    "        classifier, data=x_train, n_sampling=10, is_temporal=True\n",
    "    )\n",
    ")\n",
    "\n",
    "attr = explainer.attribute(\n",
    "    x_test,\n",
    "    sliding_window_shapes=(1,),\n",
    "    attributions_fn=abs,\n",
    "    task=\"binary\",\n",
    "    show_progress=True,\n",
    ").abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298a90b6",
   "metadata": {},
   "source": [
    "### Attributions evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d94a26",
   "metadata": {},
   "source": [
    "Since we know the true attributions, we can evaluate our computed attributions \n",
    "using our white-box metrics. For instance, we compute here the ``aur`` (area under recall):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fd8c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get true saliency\n",
    "true_saliency = hmm.true_saliency(split=\"test\").to(accelerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61505d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{aur(attr, true_saliency):.4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11303c7d",
   "metadata": {},
   "source": [
    "This is slightly better than the results reported in https://arxiv.org/pdf/2106.05303.\n",
    "\n",
    "There are however better methods than temporal_augmented_occlusion for this task. For more details, \n",
    "please refer to our ``experiments/hmm`` section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45d0093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import random\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from captum.attr import DeepLift, GradientShap, IntegratedGradients, Lime\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from typing import List\n",
    "from classifier import StateClassifierNet\n",
    "\n",
    "from tint.attr import (\n",
    "    DynaMask,\n",
    "    ExtremalMask,\n",
    "    Fit,\n",
    "    Retain,\n",
    "    TemporalAugmentedOcclusion,\n",
    "    TemporalOcclusion,\n",
    "    TimeForwardTunnel,\n",
    ")\n",
    "from tint.attr.models import (\n",
    "    ExtremalMaskNet,\n",
    "    JointFeatureGeneratorNet,\n",
    "    MaskNet,\n",
    "    RetainNet,\n",
    ")\n",
    "from tint.datasets import HMM\n",
    "from tint.metrics.white_box import (\n",
    "    aup,\n",
    "    aur,\n",
    "    information,\n",
    "    entropy,\n",
    "    roc_auc,\n",
    "    auprc,\n",
    ")\n",
    "from tint.models import MLP, RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3154b02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(check_name, trainer, model, data_module, seed, rerun_all=False):\n",
    "    checkpoint=str(seed)+\"_\"+check_name+'.ckpt'\n",
    "    if os.path.exists(checkpoint) and not rerun_all:\n",
    "        model.load_state_dict(th.load(checkpoint))\n",
    "    else:\n",
    "       trainer.fit(model, datamodule=data_module)\n",
    "       th.save(model.state_dict(), checkpoint)\n",
    "    return model\n",
    "\n",
    "def main(\n",
    "    explainers: List[str],\n",
    "    device: str = \"cpu\",\n",
    "    fold: int = 0,\n",
    "    seed: int = 42,\n",
    "    deterministic: bool = False,\n",
    "    lambda_1: float = 1.0,\n",
    "    lambda_2: float = 1.0,\n",
    "    output_file: str = \"results.csv\",\n",
    "    rerun_all=False\n",
    "):\n",
    "    # If deterministic, seed everything\n",
    "    if deterministic:\n",
    "        seed_everything(seed=seed, workers=True)\n",
    "\n",
    "    # Get accelerator and device\n",
    "    accelerator = device.split(\":\")[0]\n",
    "    device_id = 1\n",
    "    if len(device.split(\":\")) > 1:\n",
    "        device_id = [int(device.split(\":\")[1])]\n",
    "\n",
    "    # Create lock\n",
    "    lock = mp.Lock()\n",
    "\n",
    "    # Load data\n",
    "    hmm = HMM(n_folds=5, fold=fold, seed=seed)\n",
    "\n",
    "    # Create classifier\n",
    "    classifier = StateClassifierNet(\n",
    "        feature_size=3,\n",
    "        n_state=2,\n",
    "        hidden_size=200,\n",
    "        regres=True,\n",
    "        loss=\"cross_entropy\",\n",
    "        lr=0.0001,\n",
    "        l2=1e-3,\n",
    "    )\n",
    "\n",
    "    # Train classifier\n",
    "    trainer = Trainer(\n",
    "        max_epochs=500,\n",
    "        # max_epochs=3,\n",
    "        accelerator=accelerator,\n",
    "        devices=device_id,\n",
    "        deterministic=deterministic\n",
    "    )\n",
    "    # trainer.fit(classifier, datamodule=hmm)\n",
    "    classifier=get_model(check_name=\"classifier\",trainer=trainer, model=classifier, data_module=hmm, seed=seed, rerun_all=rerun_all)\n",
    "    # Get data for explainers\n",
    "    with lock:\n",
    "        x_train = hmm.preprocess(split=\"train\")[\"x\"].to(device)\n",
    "        x_test = hmm.preprocess(split=\"test\")[\"x\"].to(device)\n",
    "        y_test = hmm.preprocess(split=\"test\")[\"y\"].to(device)\n",
    "        true_saliency = hmm.true_saliency(split=\"test\").to(device)\n",
    "\n",
    "    # Switch to eval\n",
    "    classifier.eval()\n",
    "\n",
    "    # Set model to device\n",
    "    classifier.to(device)\n",
    "\n",
    "    # Disable cudnn if using cuda accelerator.\n",
    "    # Please see https://captum.ai/docs/faq#how-can-i-resolve-cudnn-rnn-backward-error-for-rnn-or-lstm-network\n",
    "    # for more information.\n",
    "    if accelerator == \"cuda\":\n",
    "        th.backends.cudnn.enabled = False\n",
    "\n",
    "    # Create dict of attributions\n",
    "    attr = dict()\n",
    "\n",
    "    if \"deep_lift\" in explainers:\n",
    "        explainer = TimeForwardTunnel(DeepLift(classifier))\n",
    "        attr[\"deep_lift\"] = explainer.attribute(\n",
    "            x_test,\n",
    "            baselines=x_test * 0,\n",
    "            task=\"binary\",\n",
    "            show_progress=True,\n",
    "        ).abs()\n",
    "\n",
    "    if \"dyna_mask\" in explainers:\n",
    "        trainer = Trainer(\n",
    "            max_epochs=1000,\n",
    "            accelerator=accelerator,\n",
    "            devices=device_id,\n",
    "            log_every_n_steps=2,\n",
    "            deterministic=deterministic,\n",
    "        )\n",
    "        mask = MaskNet(\n",
    "            forward_func=classifier,\n",
    "            perturbation=\"gaussian_blur\",\n",
    "            sigma_max=1,\n",
    "            keep_ratio=list(np.arange(0.25, 0.35, 0.01)),\n",
    "            size_reg_factor_init=0.1,\n",
    "            size_reg_factor_dilation=100,\n",
    "            time_reg_factor=1.0,\n",
    "        )\n",
    "        explainer = DynaMask(classifier)\n",
    "        _attr = explainer.attribute(\n",
    "            x_test,\n",
    "            additional_forward_args=(True,),\n",
    "            trainer=trainer,\n",
    "            mask_net=mask,\n",
    "            batch_size=100,\n",
    "            return_best_ratio=True,\n",
    "        )\n",
    "        print(f\"Best keep ratio is {_attr[1]}\")\n",
    "        attr[\"dyna_mask\"] = _attr[0].to(device)\n",
    "\n",
    "    if \"extremal_mask\" in explainers:\n",
    "        trainer = Trainer(\n",
    "            max_epochs=500,\n",
    "            # max_epochs=3,\n",
    "            accelerator=accelerator,\n",
    "            devices=device_id,\n",
    "            log_every_n_steps=2,\n",
    "            deterministic=deterministic,\n",
    "        )\n",
    "        mask = ExtremalMaskNet(\n",
    "            forward_func=classifier,\n",
    "            model=nn.Sequential(\n",
    "                RNN(\n",
    "                    input_size=x_test.shape[-1],\n",
    "                    rnn=\"gru\",\n",
    "                    hidden_size=x_test.shape[-1],\n",
    "                    bidirectional=True,\n",
    "                ),\n",
    "                MLP([2 * x_test.shape[-1], x_test.shape[-1]]),\n",
    "            ),\n",
    "            lambda_1=lambda_1,\n",
    "            lambda_2=lambda_2,\n",
    "            optim=\"adam\",\n",
    "            lr=0.01,\n",
    "        )\n",
    "        explainer = ExtremalMask(classifier)\n",
    "        _attr = explainer.attribute(\n",
    "            x_test,\n",
    "            additional_forward_args=(True,),\n",
    "            trainer=trainer,\n",
    "            mask_net=mask,\n",
    "            batch_size=100,\n",
    "        )\n",
    "        attr[\"extremal_mask\"] = _attr.to(device)\n",
    "\n",
    "    if \"fit\" in explainers:\n",
    "        generator = JointFeatureGeneratorNet(rnn_hidden_size=6)\n",
    "        trainer = Trainer(\n",
    "            max_epochs=300,\n",
    "            accelerator=accelerator,\n",
    "            devices=device_id,\n",
    "            log_every_n_steps=10,\n",
    "            deterministic=deterministic,\n",
    "        )\n",
    "        explainer = Fit(\n",
    "            classifier,\n",
    "            generator=generator,\n",
    "            datamodule=hmm,\n",
    "            trainer=trainer,\n",
    "        )\n",
    "        attr[\"fit\"] = explainer.attribute(x_test, show_progress=True)\n",
    "\n",
    "    if \"gradient_shap\" in explainers:\n",
    "        explainer = TimeForwardTunnel(GradientShap(classifier.cpu()))\n",
    "        attr[\"gradient_shap\"] = explainer.attribute(\n",
    "            x_test.cpu(),\n",
    "            baselines=th.cat([x_test.cpu() * 0, x_test.cpu()]),\n",
    "            n_samples=50,\n",
    "            stdevs=0.0001,\n",
    "            task=\"binary\",\n",
    "            show_progress=True,\n",
    "        ).abs()\n",
    "        classifier.to(device)\n",
    "\n",
    "    if \"integrated_gradients\" in explainers:\n",
    "        explainer = TimeForwardTunnel(IntegratedGradients(classifier))\n",
    "        attr[\"integrated_gradients\"] = explainer.attribute(\n",
    "            x_test,\n",
    "            baselines=x_test * 0,\n",
    "            internal_batch_size=200,\n",
    "            task=\"binary\",\n",
    "            show_progress=True,\n",
    "        ).abs()\n",
    "\n",
    "    if \"lime\" in explainers:\n",
    "        explainer = TimeForwardTunnel(Lime(classifier))\n",
    "        attr[\"lime\"] = explainer.attribute(\n",
    "            x_test,\n",
    "            task=\"binary\",\n",
    "            show_progress=True,\n",
    "        ).abs()\n",
    "\n",
    "    if \"augmented_occlusion\" in explainers:\n",
    "        explainer = TimeForwardTunnel(\n",
    "            TemporalAugmentedOcclusion(\n",
    "                classifier, data=x_train, n_sampling=10, is_temporal=True\n",
    "            )\n",
    "        )\n",
    "        attr[\"augmented_occlusion\"] = explainer.attribute(\n",
    "            x_test,\n",
    "            sliding_window_shapes=(1,),\n",
    "            attributions_fn=abs,\n",
    "            task=\"binary\",\n",
    "            show_progress=True,\n",
    "        ).abs()\n",
    "\n",
    "    if \"occlusion\" in explainers:\n",
    "        explainer = TimeForwardTunnel(TemporalOcclusion(classifier))\n",
    "        attr[\"occlusion\"] = explainer.attribute(\n",
    "            x_test,\n",
    "            sliding_window_shapes=(1,),\n",
    "            baselines=x_train.mean(0, keepdim=True),\n",
    "            attributions_fn=abs,\n",
    "            task=\"binary\",\n",
    "            show_progress=True,\n",
    "        ).abs()\n",
    "\n",
    "    if \"retain\" in explainers:\n",
    "        retain = RetainNet(\n",
    "            dim_emb=128,\n",
    "            dropout_emb=0.4,\n",
    "            dim_alpha=8,\n",
    "            dim_beta=8,\n",
    "            dropout_context=0.4,\n",
    "            dim_output=2,\n",
    "            loss=\"cross_entropy\",\n",
    "        )\n",
    "        explainer = Retain(\n",
    "            datamodule=hmm,\n",
    "            retain=retain,\n",
    "            trainer=Trainer(\n",
    "                max_epochs=50,\n",
    "                accelerator=accelerator,\n",
    "                devices=device_id,\n",
    "                deterministic=deterministic\n",
    "            ),\n",
    "        )\n",
    "        attr[\"retain\"] = (\n",
    "            explainer.attribute(x_test, target=y_test).abs().to(device)\n",
    "        )\n",
    "\n",
    "    with open(output_file, \"a\") as fp, lock:\n",
    "        for k, v in attr.items():\n",
    "            fp.write(str(seed) + \",\")\n",
    "            fp.write(str(fold) + \",\")\n",
    "            fp.write(k + \",\")\n",
    "            fp.write(str(lambda_1) + \",\")\n",
    "            fp.write(str(lambda_2) + \",\")\n",
    "            fp.write(f\"{aup(v, true_saliency):.4},\")\n",
    "            fp.write(f\"{aur(v, true_saliency):.4},\")\n",
    "            fp.write(f\"{information(v, true_saliency):.4},\")\n",
    "            fp.write(f\"{entropy(v, true_saliency):.4},\")\n",
    "            fp.write(f\"{roc_auc(v, true_saliency):.4},\")\n",
    "            fp.write(f\"{auprc(v, true_saliency):.4}\")\n",
    "            fp.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0de147b",
   "metadata": {},
   "outputs": [],
   "source": [
    "main([\"extremal_mask\"], 'cuda')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
